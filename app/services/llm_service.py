import asyncio
import aiohttp
import json
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
from app.config.llm_settings import llm_settings

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.ollama_url = llm_settings.ollama_base_url
        self.model = llm_settings.ollama_model
        self.temperature = llm_settings.temperature
        self.max_tokens = llm_settings.max_tokens
        self.timeout = llm_settings.timeout
        self.session = None
        self.memories = {}
        self.is_initialized = False
        
    async def initialize(self):
        """Inicializa conex√£o LLM com tratamento de erro melhorado"""
        try:
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
            
            logger.info(f"üîß Inicializando LLM Service...")
            logger.info(f"üìç Ollama URL: {self.ollama_url}")
            logger.info(f"ü§ñ Modelo: {self.model}")
            
            # Testa conex√£o com Ollama
            await self._test_ollama_connection()
            
            self.is_initialized = True
            logger.info(f"‚úÖ LLM Service inicializado com sucesso!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar LLM Service: {e}")
            self.is_initialized = False
            # N√£o levanta exce√ß√£o para permitir que o servi√ßo inicie
            # mas marca como n√£o inicializado
    
    async def _test_ollama_connection(self):
        """Testa conex√£o com Ollama com melhor tratamento de erro"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Tentativa {attempt + 1} de conectar ao Ollama...")
                
                # Teste 1: Verifica se Ollama est√° acess√≠vel
                async with self.session.get(f"{self.ollama_url}/api/tags") as response:
                    if response.status != 200:
                        raise Exception(f"Ollama tags endpoint retornou status {response.status}")
                    
                    tags_data = await response.json()
                    models = tags_data.get('models', [])
                    model_names = [m.get('name', '') for m in models]
                    
                    logger.info(f"Modelos dispon√≠veis: {model_names}")
                    
                    if self.model not in model_names:
                        logger.warning(f"‚ö†Ô∏è Modelo {self.model} n√£o encontrado. Usando primeiro dispon√≠vel.")
                        if model_names:
                            self.model = model_names[0]
                            logger.info(f"Usando modelo: {self.model}")
                        else:
                            raise Exception("Nenhum modelo dispon√≠vel no Ollama")
                
                # Sucesso
                return
                
            except Exception as e:
                logger.error(f"Tentativa {attempt + 1} falhou: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                else:
                    raise
    
    async def generate_response(
        self, 
        prompt: str, 
        system_message: str = None,
        session_id: str = None,
        context: Dict[str, Any] = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """Gera resposta com fallback em caso de erro"""
        
        if not self.is_initialized:
            logger.warning("LLM Service n√£o inicializado, usando resposta padr√£o")
            return self._get_fallback_response(prompt)
        
        try:
            start_time = datetime.now()
            
            # Constr√≥i mensagens
            messages = []
            
            if system_message:
                messages.append({"role": "system", "content": system_message})
            
            # Adiciona contexto da sess√£o
            if session_id and session_id in self.memories:
                memory = self.memories[session_id]
                for msg in memory[-6:]:
                    messages.append(msg)
            
            # Adiciona contexto adicional
            if context:
                context_parts = []
                if "agent_info" in context:
                    context_parts.append(f"Agente atual: {context['agent_info'].get('name', 'Desconhecido')}")
                if "user_info" in context:
                    context_parts.append(f"Usu√°rio: {context['user_info'].get('phone_number', 'Desconhecido')}")
                
                if context_parts:
                    context_str = "Contexto: " + ", ".join(context_parts)
                    messages.append({"role": "system", "content": context_str})
            
            # Adiciona prompt atual
            messages.append({"role": "user", "content": prompt})
            
            # Prepara payload
            url = f"{self.ollama_url}/api/chat"
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature or self.temperature,
                    "num_predict": max_tokens or self.max_tokens,
                    "top_k": 40,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            # Faz requisi√ß√£o
            async with self.session.post(url, json=payload) as response:
                if response.status != 200:
                    text = await response.text()
                    logger.error(f"Erro Ollama (status {response.status}): {text}")
                    return self._get_fallback_response(prompt)
                
                result = await response.json()
                
                if "error" in result:
                    logger.error(f"Ollama error: {result['error']}")
                    return self._get_fallback_response(prompt)
                
                content = result.get("message", {}).get("content", "")
                if not content:
                    return self._get_fallback_response(prompt)
                
                # Salva na mem√≥ria
                if session_id:
                    if session_id not in self.memories:
                        self.memories[session_id] = []
                    self.memories[session_id].append({"role": "user", "content": prompt})
                    self.memories[session_id].append({"role": "assistant", "content": content})
                    self._trim_memory(session_id)
                
                return content.strip()
                
        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}", exc_info=True)
            return self._get_fallback_response(prompt)
    
    def _get_fallback_response(self, prompt: str) -> str:
        """Resposta fallback quando LLM n√£o est√° dispon√≠vel"""
        prompt_lower = prompt.lower()
        
        # Respostas mais naturais e variadas
        greetings = ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa"]
        if any(word in prompt_lower for word in greetings):
            responses = [
                "Oi! Tudo bem? üòä Como posso te ajudar hoje?",
                "Opa! Que bom te ver por aqui! O que voc√™ precisa?",
                "Oi oi! Como voc√™ t√°? Precisa de alguma coisa?",
                "Hey! Tudo certo? Me conta como posso ajudar!",
                "Ol√°! Seja bem-vindo(a)! Em que posso ser √∫til?"
            ]
            import random
            return random.choice(responses)
        
        elif any(word in prompt_lower for word in ["menu", "op√ß√µes", "opcoes", "ajuda", "o que voc√™ faz"]):
            return """Claro! Eu posso te ajudar com v√°rias coisas:

    üìä **Dados e Relat√≥rios** - Vendas, m√©tricas, dashboards
    üîß **Suporte T√©cnico** - Problemas, erros, d√∫vidas
    üìÖ **Agendamentos** - Reuni√µes, compromissos
    üí¨ **Bate-papo** - Qualquer outra coisa!

    O que voc√™ precisa? üòä"""
        
        elif any(word in prompt_lower for word in ["dados", "relat√≥rio", "relatorio", "vendas", "dashboard", "m√©trica", "metrica"]):
            return """Legal! Vou puxar essas informa√ß√µes pra voc√™! üìä

    Voc√™ quer ver:
    - Vendas do m√™s?
    - Comparativo com m√™s anterior?
    - Performance geral?
    - Ou algum dado espec√≠fico?

    Me conta o que precisa!"""
        
        elif any(word in prompt_lower for word in ["erro", "problema", "bug", "n√£o funciona", "nao funciona", "travou"]):
            return """Poxa, que chato! Vamos resolver isso juntos üîß

    Me conta:
    - O que aconteceu exatamente?
    - Quando come√ßou o problema?
    - J√° tentou reiniciar?

    Com essas infos consigo te ajudar melhor!"""
        
        elif any(word in prompt_lower for word in ["obrigado", "obrigada", "valeu", "thanks", "brigado"]):
            responses = [
                "Por nada! Sempre que precisar, t√¥ aqui! üòä",
                "Imagina! Foi um prazer ajudar! ü§ó",
                "Que isso! Conta comigo sempre! ‚ú®",
                "De nada! Volte sempre que precisar!"
            ]
            import random
            return random.choice(responses)
        
        elif any(word in prompt_lower for word in ["tchau", "at√©", "ate", "adeus", "bye", "xau"]):
            responses = [
                "Tchau! Foi √≥timo falar com voc√™! At√© mais! üëã",
                "At√© logo! Se cuida! üòä",
                "Valeu pela conversa! At√© a pr√≥xima! ‚ú®",
                "Tchau tchau! Volte sempre! ü§ó"
            ]
            import random
            return random.choice(responses)
        
        else:
            # Resposta gen√©rica mais natural
            responses = [
                "Hmm, n√£o entendi muito bem... Pode me explicar de outro jeito? üòä",
                "Opa, acho que n√£o captei. Pode dar mais detalhes?",
                "Desculpa, n√£o entendi direito. Voc√™ quer dados, suporte ou marcar algo?",
                "Poxa, n√£o peguei bem o que voc√™ precisa. Me conta mais?"
            ]
            import random
            return random.choice(responses)
    
    async def classify_intent(self, message: str, session_id: str = None) -> Dict[str, Any]:
        """Classifica inten√ß√£o com fallback"""
        
        if self.is_initialized:
            system_prompt = """Classifique a mensagem em uma destas categorias:
- "reception": sauda√ß√µes, cumprimentos
- "data_query": dados, relat√≥rios, m√©tricas
- "technical_support": problemas, erros, bugs
- "scheduling": agendamentos, reuni√µes
- "general_chat": outros

Responda APENAS em JSON: {"intent": "categoria", "confidence": 0.0-1.0, "reasoning": "explica√ß√£o"}"""

            try:
                response = await self.generate_response(
                    f"Classifique: '{message}'",
                    system_prompt,
                    session_id,
                    temperature=0.3,
                    max_tokens=150
                )
                
                # Tenta extrair JSON
                response = response.replace("```json", "").replace("```", "").strip()
                start = response.find('{')
                end = response.rfind('}') + 1
                
                if start != -1 and end > start:
                    json_str = response[start:end]
                    result = json.loads(json_str)
                    
                    if all(key in result for key in ["intent", "confidence"]):
                        result["confidence"] = float(result["confidence"])
                        return result
                        
            except Exception as e:
                logger.error(f"Erro na classifica√ß√£o LLM: {e}")
        
        # Fallback com classifica√ß√£o por palavras-chave
        return self._classify_by_keywords(message)
    
    def _classify_by_keywords(self, message: str) -> Dict[str, Any]:
        """Classifica√ß√£o fallback por palavras-chave"""
        message_lower = message.lower()
        
        # Detec√ß√£o mais natural e abrangente
        greetings = ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa", "e ai", "eai", "fala"]
        data_keywords = ["relat√≥rio", "relatorio", "dados", "dashboard", "vendas", "faturamento", "m√©trica", "metrica", "kpi", "n√∫meros", "numeros", "estat√≠stica", "estatistica"]
        support_keywords = ["erro", "problema", "bug", "n√£o funciona", "nao funciona", "travou", "lento", "parou", "ajuda t√©cnica", "suporte"]
        scheduling_keywords = ["agendar", "marcar", "reuni√£o", "reuniao", "hor√°rio", "horario", "calend√°rio", "calendario", "compromisso"]
        
        # Classifica√ß√£o com confian√ßa vari√°vel
        if any(word in message_lower for word in greetings):
            return {"intent": "reception", "confidence": 0.95, "reasoning": "Sauda√ß√£o identificada"}
        elif any(word in message_lower for word in data_keywords):
            return {"intent": "data_query", "confidence": 0.85, "reasoning": "Consulta de dados"}
        elif any(word in message_lower for word in support_keywords):
            return {"intent": "technical_support", "confidence": 0.85, "reasoning": "Problema t√©cnico"}
        elif any(word in message_lower for word in scheduling_keywords):
            return {"intent": "scheduling", "confidence": 0.85, "reasoning": "Agendamento"}
        else:
            # Se n√£o identificar claramente, vai para recep√ß√£o para perguntar melhor
            return {"intent": "reception", "confidence": 0.4, "reasoning": "Inten√ß√£o n√£o clara"}
    
    def _trim_memory(self, session_id: str, max_messages: int = 10):
        """Mant√©m apenas as √∫ltimas N mensagens"""
        if session_id in self.memories and len(self.memories[session_id]) > max_messages:
            self.memories[session_id] = self.memories[session_id][-max_messages:]
    
    async def cleanup(self):
        """Limpa recursos"""
        if self.session:
            await self.session.close()
        self.memories.clear()
        self.is_initialized = False
        logger.info("LLM Service cleaned up")

    async def get_service_status(self) -> Dict[str, Any]:
        """Retorna status do servi√ßo LLM"""
        try:
            status = {
                "status": "online" if self.is_initialized else "offline",
                "ollama_url": self.ollama_url,
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "initialized": self.is_initialized,
                "timestamp": datetime.now().isoformat()
            }
            
            # Testa conex√£o com Ollama se inicializado
            if self.is_initialized and self.session:
                try:
                    async with self.session.get(f"{self.ollama_url}/api/tags", timeout=aiohttp.ClientTimeout(total=2)) as response:
                        if response.status == 200:
                            data = await response.json()
                            models = [m.get('name', '') for m in data.get('models', [])]
                            status["available_models"] = models
                            status["model_available"] = self.model in models
                        else:
                            status["status"] = "degraded"
                except Exception as e:
                    logger.warning(f"Could not check Ollama status: {e}")
                    status["status"] = "degraded"
                    status["error"] = str(e)
            
            return status
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }