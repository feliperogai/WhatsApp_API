import asyncio
import aiohttp
import json
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import traceback
from app.config.llm_settings import llm_settings

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.ollama_url = llm_settings.ollama_base_url
        self.model = llm_settings.ollama_model
        self.temperature = llm_settings.temperature
        self.max_tokens = llm_settings.max_tokens
        self.timeout = llm_settings.timeout
        self.session = None
        self.memories = {}
        self.is_initialized = False
        self.connection_error = None
        self.last_test_time = None
        self.last_test_result = None
        
    async def initialize(self):
        """Inicializa conex√£o LLM com tratamento de erro melhorado"""
        try:
            logger.info("="*60)
            logger.info("üîß Inicializando LLM Service...")
            logger.info(f"üìç Ollama URL: {self.ollama_url}")
            logger.info(f"ü§ñ Modelo Configurado: {self.model}")
            logger.info(f"üå°Ô∏è Temperature: {self.temperature}")
            logger.info(f"üìè Max Tokens: {self.max_tokens}")
            logger.info(f"‚è±Ô∏è Timeout: {self.timeout}s")
            logger.info("="*60)
            
            # Cria sess√£o HTTP
            timeout_config = aiohttp.ClientTimeout(total=self.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout_config)
            
            # Testa conex√£o com Ollama
            connection_ok = await self._test_ollama_connection()
            
            if connection_ok:
                self.is_initialized = True
                logger.info("‚úÖ LLM Service inicializado com sucesso!")
                logger.info(f"‚úÖ Modelo {self.model} est√° dispon√≠vel e funcionando")
            else:
                self.is_initialized = False
                logger.warning("‚ö†Ô∏è LLM Service inicializado em modo fallback (Ollama n√£o dispon√≠vel)")
                
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao inicializar LLM Service: {e}")
            logger.error(traceback.format_exc())
            self.is_initialized = False
            self.connection_error = str(e)
    
    async def _test_ollama_connection(self):
        """Testa conex√£o com Ollama com debug detalhado"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üîç Tentativa {attempt + 1}/{max_retries} de conectar ao Ollama...")
                
                # Teste 1: Verifica se endpoint est√° acess√≠vel
                test_url = f"{self.ollama_url}/api/tags"
                logger.debug(f"Testing URL: {test_url}")
                
                async with self.session.get(test_url) as response:
                    logger.debug(f"Response status: {response.status}")
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"‚ùå Ollama retornou status {response.status}: {error_text}")
                        raise Exception(f"Ollama endpoint returned status {response.status}")
                    
                    tags_data = await response.json()
                    models = tags_data.get('models', [])
                    model_names = [m.get('name', '') for m in models]
                    
                    logger.info(f"üìã Modelos dispon√≠veis no Ollama: {model_names}")
                    
                    # Verifica se o modelo configurado existe
                    if self.model not in model_names:
                        logger.warning(f"‚ö†Ô∏è Modelo {self.model} n√£o encontrado!")
                        if model_names:
                            # Tenta usar um modelo alternativo
                            for preferred in ['llama3.1:8b', 'llama3:latest', 'llama2:latest']:
                                if preferred in model_names:
                                    self.model = preferred
                                    logger.info(f"‚úÖ Usando modelo alternativo: {self.model}")
                                    break
                            else:
                                # Usa o primeiro dispon√≠vel
                                self.model = model_names[0]
                                logger.info(f"‚úÖ Usando primeiro modelo dispon√≠vel: {self.model}")
                        else:
                            raise Exception("Nenhum modelo dispon√≠vel no Ollama")
                
                # Teste 2: Tenta fazer uma chamada simples
                logger.info("üß™ Testando gera√ß√£o com o modelo...")
                test_payload = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": "test"}],
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 10
                    }
                }
                
                test_url = f"{self.ollama_url}/api/chat"
                async with self.session.post(test_url, json=test_payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"‚úÖ Teste de gera√ß√£o bem-sucedido!")
                        self.last_test_time = datetime.now()
                        self.last_test_result = "success"
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Falha no teste de gera√ß√£o: {error_text}")
                        self.last_test_result = f"generation_failed: {response.status}"
                
            except aiohttp.ClientError as e:
                logger.error(f"‚ùå Erro de conex√£o (tentativa {attempt + 1}): {type(e).__name__}: {str(e)}")
                self.connection_error = f"Connection error: {str(e)}"
                
            except asyncio.TimeoutError:
                logger.error(f"‚ùå Timeout ao conectar com Ollama (tentativa {attempt + 1})")
                self.connection_error = "Timeout connecting to Ollama"
                
            except Exception as e:
                logger.error(f"‚ùå Erro inesperado (tentativa {attempt + 1}): {type(e).__name__}: {str(e)}")
                logger.error(traceback.format_exc())
                self.connection_error = str(e)
            
            if attempt < max_retries - 1:
                logger.info(f"‚è≥ Aguardando {retry_delay}s antes da pr√≥xima tentativa...")
                await asyncio.sleep(retry_delay)
        
        logger.error("‚ùå Todas as tentativas de conex√£o com Ollama falharam!")
        return False
    
    async def generate_response(
        self, 
        prompt: str, 
        system_message: str = None,
        session_id: str = None,
        context: Dict[str, Any] = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """Gera resposta com fallback robusto"""
        
        logger.debug(f"üìù Generating response for prompt: {prompt[:50]}...")
        
        # Se n√£o est√° inicializado ou Ollama n√£o est√° dispon√≠vel, usa fallback
        if not self.is_initialized or not self.session:
            logger.warning("‚ö†Ô∏è LLM n√£o inicializado, usando fallback")
            return self._get_fallback_response(prompt)
        
        try:
            start_time = datetime.now()
            
            # Constr√≥i mensagens
            messages = []
            
            if system_message:
                messages.append({"role": "system", "content": system_message})
                logger.debug(f"System message: {system_message[:100]}...")
            
            # Adiciona contexto da sess√£o
            if session_id and session_id in self.memories:
                memory = self.memories[session_id]
                # Pega apenas as √∫ltimas 6 mensagens para n√£o exceder o contexto
                for msg in memory[-6:]:
                    messages.append(msg)
                logger.debug(f"Added {len(memory[-6:])} messages from memory")
            
            # Adiciona contexto adicional
            if context:
                context_parts = []
                if "agent_info" in context:
                    agent_name = context['agent_info'].get('name', 'Desconhecido')
                    context_parts.append(f"Voc√™ est√° atuando como: {agent_name}")
                if "user_info" in context:
                    phone = context['user_info'].get('phone_number', 'Desconhecido')
                    context_parts.append(f"Conversando com: {phone}")
                
                if context_parts:
                    context_str = "\n".join(context_parts)
                    messages.append({"role": "system", "content": context_str})
                    logger.debug(f"Added context: {context_str}")
            
            # Adiciona prompt atual
            messages.append({"role": "user", "content": prompt})
            
            # Prepara payload
            url = f"{self.ollama_url}/api/chat"
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature or self.temperature,
                    "num_predict": max_tokens or self.max_tokens,
                    "top_k": 40,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            logger.debug(f"üöÄ Sending request to Ollama...")
            logger.debug(f"URL: {url}")
            logger.debug(f"Model: {self.model}")
            
            # Faz requisi√ß√£o
            async with self.session.post(url, json=payload) as response:
                response_text = await response.text()
                logger.debug(f"Response status: {response.status}")
                
                if response.status != 200:
                    logger.error(f"‚ùå Erro Ollama (status {response.status}): {response_text}")
                    return self._get_fallback_response(prompt)
                
                try:
                    result = json.loads(response_text)
                except json.JSONDecodeError:
                    logger.error(f"‚ùå Invalid JSON response: {response_text[:200]}...")
                    return self._get_fallback_response(prompt)
                
                if "error" in result:
                    logger.error(f"‚ùå Ollama error: {result['error']}")
                    return self._get_fallback_response(prompt)
                
                content = result.get("message", {}).get("content", "")
                if not content:
                    logger.error("‚ùå Empty response from Ollama")
                    return self._get_fallback_response(prompt)
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"‚úÖ LLM response generated in {elapsed:.2f}s")
                logger.debug(f"Response preview: {content[:100]}...")
                
                # Salva na mem√≥ria da sess√£o
                if session_id:
                    if session_id not in self.memories:
                        self.memories[session_id] = []
                    self.memories[session_id].append({"role": "user", "content": prompt})
                    self.memories[session_id].append({"role": "assistant", "content": content})
                    self._trim_memory(session_id)
                
                return content.strip()
                
        except aiohttp.ClientError as e:
            logger.error(f"‚ùå Network error: {type(e).__name__}: {str(e)}")
            return self._get_fallback_response(prompt)
            
        except asyncio.TimeoutError:
            logger.error(f"‚ùå Timeout ap√≥s {self.timeout}s")
            return self._get_fallback_response(prompt)
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao gerar resposta: {type(e).__name__}: {str(e)}")
            logger.error(traceback.format_exc())
            return self._get_fallback_response(prompt)
    
    def _get_fallback_response(self, prompt: str) -> str:
        """Resposta fallback natural e variada quando LLM n√£o est√° dispon√≠vel"""
        logger.info(f"üîÑ Using fallback response for: {prompt[:50]}...")
        prompt_lower = prompt.lower()
        import random
        
        # Respostas mais naturais e variadas por categoria
        
        # Sauda√ß√µes
        greetings = ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa", "eae", "e ai", "fala", "salve"]
        if any(word in prompt_lower for word in greetings):
            responses = [
                "Opa! E a√≠, tudo bem? üòä",
                "Oi oi! Como voc√™ t√°?",
                "Fala! Tudo certo a√≠?",
                "Hey! Que bom te ver por aqui!",
                "Oi! Tava esperando voc√™ aparecer! Como t√°?",
                "E a√≠! Beleza? Como posso ajudar?",
                "Salve! Tudo tranquilo?",
                "Ol√°! Como t√° seu dia hoje?",
                "Opa, tudo bem? Que legal voc√™ por aqui!"
            ]
            # Para hor√°rios espec√≠ficos
            from datetime import datetime
            hour = datetime.now().hour
            if 5 <= hour < 12 and "bom dia" in prompt_lower:
                responses.extend([
                    "Bom dia! Acordou cedo hein! Como t√°?",
                    "Bom dia! ‚òÄÔ∏è J√° tomou caf√©?",
                    "Bom dia! Que seu dia seja incr√≠vel!"
                ])
            elif 12 <= hour < 18 and "boa tarde" in prompt_lower:
                responses.extend([
                    "Boa tarde! Como foi sua manh√£?",
                    "Boa tarde! T√° calor a√≠?",
                    "Boa tarde! J√° almo√ßou?"
                ])
            elif "boa noite" in prompt_lower:
                responses.extend([
                    "Boa noite! Como foi seu dia?",
                    "Boa noite! üåô Tudo bem?",
                    "Boa noite! Ainda trabalhando?"
                ])
            return random.choice(responses)
        
        # Pedidos de ajuda/servi√ßos
        elif any(word in prompt_lower for word in ["ajuda", "ajudar", "servi√ßo", "servi√ßos", "o que voc√™ faz", "pode fazer", "consegue"]):
            responses = [
                "Claro! Eu ajudo com v√°rias coisas: relat√≥rios, dados da empresa, problemas t√©cnicos, agendamentos... O que voc√™ precisa?",
                "Opa, t√¥ aqui pra isso! Posso puxar relat√≥rios, resolver problemas t√©cnicos, marcar reuni√µes... Me conta o que precisa!",
                "Ah, eu fa√ßo um monte de coisa! Dados, suporte, agenda... Mas me diz, o que t√° precisando agora?",
                "Consigo te ajudar com relat√≥rios e dados, resolver problemas t√©cnicos, organizar agenda... O que seria bom pra voc√™?"
            ]
            return random.choice(responses)
        
        # Menu (pessoa insiste em formato menu)
        elif "menu" in prompt_lower:
            responses = [
                "Ent√£o, n√£o tenho bem um 'menu' haha, mas posso te ajudar com dados e relat√≥rios, problemas t√©cnicos, agendamentos... O que voc√™ precisa?",
                "Menu? üòÑ Bom, eu ajudo com relat√≥rios da empresa, suporte t√©cnico, marco reuni√µes... Qual dessas coisas voc√™ t√° precisando?",
                "Hmm menu... Deixa eu pensar! Fa√ßo relat√≥rios, resolvo bugs, organizo agenda... Te interessa alguma coisa espec√≠fica?"
            ]
            return random.choice(responses)
        
        # Dados/Relat√≥rios
        elif any(word in prompt_lower for word in ["dados", "relat√≥rio", "relatorio", "vendas", "dashboard", "m√©trica", "kpi"]):
            responses = [
                "Ah, voc√™ quer ver dados! Legal! Me conta mais: vendas, clientes, performance... O que seria √∫til pra voc√™?",
                "Show! Adoro mostrar n√∫meros! üìä Quer ver vendas? Clientes? Ou alguma m√©trica espec√≠fica?",
                "Opa, vamos aos dados! O que voc√™ quer saber? Vendas do m√™s? Comparativo? Performance?",
                "Relat√≥rios! Boa! Temos v√°rias op√ß√µes... Vendas, clientes, KPIs... Por onde quer come√ßar?"
            ]
            return random.choice(responses)
        
        # Problemas t√©cnicos
        elif any(word in prompt_lower for word in ["erro", "problema", "bug", "n√£o funciona", "travou", "lento"]):
            responses = [
                "Eita, que chato! Me conta direitinho o que t√° acontecendo que eu te ajudo!",
                "Poxa, problema t√©cnico √© fogo! O que t√° dando erro a√≠?",
                "Xiii, vamos resolver isso! Me explica o que aconteceu?",
                "Problema? Calma que a gente resolve! O que t√° pegando?"
            ]
            return random.choice(responses)
        
        # Agendamentos
        elif any(word in prompt_lower for word in ["agendar", "marcar", "reuni√£o", "hor√°rio", "agenda"]):
            responses = [
                "Beleza! Vamos marcar! Que tipo de compromisso voc√™ quer agendar?",
                "Show! Me conta: √© reuni√£o? Call? Presencial? Quando seria bom?",
                "Opa, vamos organizar sua agenda! O que precisa marcar?",
                "Legal! Agendamento! √â reuni√£o de trabalho? Me d√° mais detalhes!"
            ]
            return random.choice(responses)
        
        # Agradecimentos
        elif any(word in prompt_lower for word in ["obrigado", "obrigada", "valeu", "thanks", "agrade√ßo"]):
            responses = [
                "Imagina! Sempre que precisar! üòä",
                "Por nada! Foi um prazer!",
                "Que isso! Tamo junto! ü§ù",
                "De nada! Conta comigo!",
                "Valeu voc√™! Fico feliz em ajudar!",
                "Nada! Precisando, s√≥ chamar!"
            ]
            return random.choice(responses)
        
        # Despedidas
        elif any(word in prompt_lower for word in ["tchau", "at√©", "adeus", "bye", "xau", "flw", "falou"]):
            responses = [
                "Tchau! Foi √≥timo falar com voc√™! üëã",
                "At√© mais! Se cuida!",
                "Falou! Boa sorte a√≠! ‚ú®",
                "Tchau tchau! Aparece mais!",
                "At√©! Qualquer coisa me chama!",
                "Valeu pela conversa! At√© a pr√≥xima!"
            ]
            return random.choice(responses)
        
        # Teste
        elif any(word in prompt_lower for word in ["teste", "testando", "test"]):
            responses = [
                "Recebi seu teste! T√° tudo funcionando! üß™",
                "Teste recebido! T√¥ aqui, pode falar!",
                "Testando 1, 2, 3... T√° me ouvindo bem? üòÑ"
            ]
            return random.choice(responses)
        
        # Respostas gen√©ricas (n√£o entendeu)
        else:
            responses = [
                "Hmm, n√£o entendi bem... Pode me explicar melhor?",
                "Opa, acho que n√£o captei. Pode falar de outro jeito?",
                "Desculpa, n√£o peguei essa. Me conta mais?",
                "Putz, n√£o entendi direito. Voc√™ quer dados, suporte ou marcar algo?",
                "Eita, me perdi aqui! üòÖ Pode repetir?",
                "N√£o entendi muito bem, mas t√¥ aqui pra ajudar! Me explica melhor?",
                f"Interessante voc√™ falar sobre '{prompt[:30]}{'...' if len(prompt) > 30 else ''}'. Mas n√£o entendi o que precisa. Pode elaborar?"
            ]
            return random.choice(responses)
    
    def _classify_by_keywords(self, message: str) -> Dict[str, Any]:
        """Classifica√ß√£o fallback por palavras-chave melhorada"""
        message_lower = message.lower()
        
        # Padr√µes mais abrangentes
        patterns = {
            "reception": {
                "keywords": ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa", 
                           "e ai", "eai", "fala", "al√¥", "alo", "prezado", "caro", "tchau", "at√©",
                           "obrigado", "valeu", "flw", "falou"],
                "confidence": 0.95
            },
            "data_query": {
                "keywords": ["relat√≥rio", "relatorio", "dados", "dashboard", "vendas", "faturamento", 
                           "m√©trica", "metrica", "kpi", "n√∫meros", "numeros", "estat√≠stica", 
                           "estatistica", "an√°lise", "analise", "gr√°fico", "grafico", "planilha",
                           "excel", "csv", "exportar", "resultado", "performance", "indicador"],
                "confidence": 0.85
            },
            "technical_support": {
                "keywords": ["erro", "problema", "bug", "n√£o funciona", "nao funciona", "travou", 
                           "lento", "parou", "ajuda t√©cnica", "suporte", "falha", "crash", "down",
                           "offline", "timeout", "conex√£o", "conexao", "acesso negado", "senha",
                           "login", "autentica√ß√£o", "autenticacao", "permiss√£o", "permissao"],
                "confidence": 0.85
            },
            "scheduling": {
                "keywords": ["agendar", "marcar", "reuni√£o", "reuniao", "hor√°rio", "horario", 
                           "calend√°rio", "calendario", "compromisso", "disponibilidade", "agenda",
                           "remarcar", "cancelar", "adiar", "confirmar", "meeting", "call",
                           "videoconfer√™ncia", "videoconferencia"],
                "confidence": 0.85
            }
        }
        
        # Verifica cada padr√£o
        for intent, pattern in patterns.items():
            if any(keyword in message_lower for keyword in pattern["keywords"]):
                reasoning = f"Detectada palavra-chave relacionada a {intent}"
                logger.info(f"‚úÖ Keyword match for intent: {intent}")
                return {
                    "intent": intent,
                    "confidence": pattern["confidence"],
                    "reasoning": reasoning
                }
        
        # Se n√£o encontrar padr√£o claro, classifica como general_chat
        logger.info("‚ÑπÔ∏è No clear pattern found, classifying as general_chat")
        return {
            "intent": "general_chat",
            "confidence": 0.5,
            "reasoning": "Nenhum padr√£o espec√≠fico detectado"
        }
    
    def _trim_memory(self, session_id: str, max_messages: int = 10):
        """Mant√©m apenas as √∫ltimas N mensagens na mem√≥ria"""
        if session_id in self.memories and len(self.memories[session_id]) > max_messages:
            self.memories[session_id] = self.memories[session_id][-max_messages:]
            logger.debug(f"Trimmed memory for session {session_id} to {max_messages} messages")
    
    async def cleanup(self):
        """Limpa recursos"""
        logger.info("üßπ Cleaning up LLM Service...")
        
        if self.session:
            await self.session.close()
            
        self.memories.clear()
        self.is_initialized = False
        
        logger.info("‚úÖ LLM Service cleaned up")

    async def get_service_status(self) -> Dict[str, Any]:
        """Retorna status detalhado do servi√ßo LLM"""
        try:
            status = {
                "status": "online" if self.is_initialized else "offline",
                "initialized": self.is_initialized,
                "ollama_url": self.ollama_url,
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "timeout": self.timeout,
                "timestamp": datetime.now().isoformat(),
                "memory_sessions": len(self.memories),
                "connection_error": self.connection_error,
                "last_test": {
                    "time": self.last_test_time.isoformat() if self.last_test_time else None,
                    "result": self.last_test_result
                }
            }
            
            # Testa conex√£o atual se inicializado
            if self.is_initialized and self.session:
                try:
                    # Teste r√°pido de conectividade
                    test_start = datetime.now()
                    async with self.session.get(
                        f"{self.ollama_url}/api/tags", 
                        timeout=aiohttp.ClientTimeout(total=2)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            models = [m.get('name', '') for m in data.get('models', [])]
                            status["available_models"] = models
                            status["model_available"] = self.model in models
                            status["connectivity"] = "connected"
                            status["ping_ms"] = int((datetime.now() - test_start).total_seconds() * 1000)
                        else:
                            status["connectivity"] = "error"
                            status["connectivity_error"] = f"HTTP {response.status}"
                except asyncio.TimeoutError:
                    status["connectivity"] = "timeout"
                except Exception as e:
                    status["connectivity"] = "error"
                    status["connectivity_error"] = str(e)
            else:
                status["connectivity"] = "not_initialized"
            
            return status
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def classify_intent(self, message: str) -> Dict[str, Any]:
        """Classifica a inten√ß√£o da mensagem usando palavras-chave (p√∫blico para orquestrador)"""
        return self._classify_by_keywords(message)