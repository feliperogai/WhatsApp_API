import asyncio
import aiohttp
import json
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import traceback
from app.config.llm_settings import llm_settings

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.ollama_url = llm_settings.ollama_base_url
        self.model = llm_settings.ollama_model
        self.temperature = llm_settings.temperature
        self.max_tokens = llm_settings.max_tokens
        self.timeout = llm_settings.timeout
        self.session = None
        self.memories = {}
        self.is_initialized = False
        self.connection_error = None
        self.last_test_time = None
        self.last_test_result = None
        
    async def initialize(self):
        """Inicializa conex√£o LLM com tratamento de erro melhorado"""
        try:
            logger.info("="*60)
            logger.info("üîß Inicializando LLM Service...")
            logger.info(f"üìç Ollama URL: {self.ollama_url}")
            logger.info(f"ü§ñ Modelo Configurado: {self.model}")
            logger.info(f"üå°Ô∏è Temperature: {self.temperature}")
            logger.info(f"üìè Max Tokens: {self.max_tokens}")
            logger.info(f"‚è±Ô∏è Timeout: {self.timeout}s")
            logger.info("="*60)
            
            # Cria sess√£o HTTP
            timeout_config = aiohttp.ClientTimeout(total=self.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout_config)
            
            # Testa conex√£o com Ollama
            connection_ok = await self._test_ollama_connection()
            
            if connection_ok:
                self.is_initialized = True
                logger.info("‚úÖ LLM Service inicializado com sucesso!")
                logger.info(f"‚úÖ Modelo {self.model} est√° dispon√≠vel e funcionando")
            else:
                self.is_initialized = False
                logger.warning("‚ö†Ô∏è LLM Service inicializado em modo fallback (Ollama n√£o dispon√≠vel)")
                
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao inicializar LLM Service: {e}")
            logger.error(traceback.format_exc())
            self.is_initialized = False
            self.connection_error = str(e)
    
    async def _test_ollama_connection(self):
        """Testa conex√£o com Ollama com debug detalhado"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üîç Tentativa {attempt + 1}/{max_retries} de conectar ao Ollama...")
                
                # Teste 1: Verifica se endpoint est√° acess√≠vel
                test_url = f"{self.ollama_url}/api/tags"
                logger.debug(f"Testing URL: {test_url}")
                
                async with self.session.get(test_url) as response:
                    logger.debug(f"Response status: {response.status}")
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"‚ùå Ollama retornou status {response.status}: {error_text}")
                        raise Exception(f"Ollama endpoint returned status {response.status}")
                    
                    tags_data = await response.json()
                    models = tags_data.get('models', [])
                    model_names = [m.get('name', '') for m in models]
                    
                    logger.info(f"üìã Modelos dispon√≠veis no Ollama: {model_names}")
                    
                    # Verifica se o modelo configurado existe
                    if self.model not in model_names:
                        logger.warning(f"‚ö†Ô∏è Modelo {self.model} n√£o encontrado!")
                        if model_names:
                            # Tenta usar um modelo alternativo
                            for preferred in ['llama3.1:8b', 'llama3:latest', 'llama2:latest']:
                                if preferred in model_names:
                                    self.model = preferred
                                    logger.info(f"‚úÖ Usando modelo alternativo: {self.model}")
                                    break
                            else:
                                # Usa o primeiro dispon√≠vel
                                self.model = model_names[0]
                                logger.info(f"‚úÖ Usando primeiro modelo dispon√≠vel: {self.model}")
                        else:
                            raise Exception("Nenhum modelo dispon√≠vel no Ollama")
                
                # Teste 2: Tenta fazer uma chamada simples
                logger.info("üß™ Testando gera√ß√£o com o modelo...")
                test_payload = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": "test"}],
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 10
                    }
                }
                
                test_url = f"{self.ollama_url}/api/chat"
                async with self.session.post(test_url, json=test_payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"‚úÖ Teste de gera√ß√£o bem-sucedido!")
                        self.last_test_time = datetime.now()
                        self.last_test_result = "success"
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Falha no teste de gera√ß√£o: {error_text}")
                        self.last_test_result = f"generation_failed: {response.status}"
                
            except aiohttp.ClientError as e:
                logger.error(f"‚ùå Erro de conex√£o (tentativa {attempt + 1}): {type(e).__name__}: {str(e)}")
                self.connection_error = f"Connection error: {str(e)}"
                
            except asyncio.TimeoutError:
                logger.error(f"‚ùå Timeout ao conectar com Ollama (tentativa {attempt + 1})")
                self.connection_error = "Timeout connecting to Ollama"
                
            except Exception as e:
                logger.error(f"‚ùå Erro inesperado (tentativa {attempt + 1}): {type(e).__name__}: {str(e)}")
                logger.error(traceback.format_exc())
                self.connection_error = str(e)
            
            if attempt < max_retries - 1:
                logger.info(f"‚è≥ Aguardando {retry_delay}s antes da pr√≥xima tentativa...")
                await asyncio.sleep(retry_delay)
        
        logger.error("‚ùå Todas as tentativas de conex√£o com Ollama falharam!")
        return False
    
    async def generate_response(
        self, 
        prompt: str, 
        system_message: str = None,
        session_id: str = None,
        context: Dict[str, Any] = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """Gera resposta com fallback robusto"""
        
        logger.debug(f"üìù Generating response for prompt: {prompt[:50]}...")
        
        # Se n√£o est√° inicializado ou Ollama n√£o est√° dispon√≠vel, usa fallback
        if not self.is_initialized or not self.session:
            logger.warning("‚ö†Ô∏è LLM n√£o inicializado, usando fallback")
            return self._get_fallback_response(prompt)
        
        try:
            start_time = datetime.now()
            
            # Constr√≥i mensagens
            messages = []
            
            if system_message:
                messages.append({"role": "system", "content": system_message})
                logger.debug(f"System message: {system_message[:100]}...")
            
            # Adiciona contexto da sess√£o
            if session_id and session_id in self.memories:
                memory = self.memories[session_id]
                # Pega apenas as √∫ltimas 6 mensagens para n√£o exceder o contexto
                for msg in memory[-6:]:
                    messages.append(msg)
                logger.debug(f"Added {len(memory[-6:])} messages from memory")
            
            # Adiciona contexto adicional
            if context:
                context_parts = []
                if "agent_info" in context:
                    agent_name = context['agent_info'].get('name', 'Desconhecido')
                    context_parts.append(f"Voc√™ est√° atuando como: {agent_name}")
                if "user_info" in context:
                    phone = context['user_info'].get('phone_number', 'Desconhecido')
                    context_parts.append(f"Conversando com: {phone}")
                
                if context_parts:
                    context_str = "\n".join(context_parts)
                    messages.append({"role": "system", "content": context_str})
                    logger.debug(f"Added context: {context_str}")
            
            # Adiciona prompt atual
            messages.append({"role": "user", "content": prompt})
            
            # Prepara payload
            url = f"{self.ollama_url}/api/chat"
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature or self.temperature,
                    "num_predict": max_tokens or self.max_tokens,
                    "top_k": 40,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            logger.debug(f"üöÄ Sending request to Ollama...")
            logger.debug(f"URL: {url}")
            logger.debug(f"Model: {self.model}")
            
            # Faz requisi√ß√£o
            async with self.session.post(url, json=payload) as response:
                response_text = await response.text()
                logger.debug(f"Response status: {response.status}")
                
                if response.status != 200:
                    logger.error(f"‚ùå Erro Ollama (status {response.status}): {response_text}")
                    return self._get_fallback_response(prompt)
                
                try:
                    result = json.loads(response_text)
                except json.JSONDecodeError:
                    logger.error(f"‚ùå Invalid JSON response: {response_text[:200]}...")
                    return self._get_fallback_response(prompt)
                
                if "error" in result:
                    logger.error(f"‚ùå Ollama error: {result['error']}")
                    return self._get_fallback_response(prompt)
                
                content = result.get("message", {}).get("content", "")
                if not content:
                    logger.error("‚ùå Empty response from Ollama")
                    return self._get_fallback_response(prompt)
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"‚úÖ LLM response generated in {elapsed:.2f}s")
                logger.debug(f"Response preview: {content[:100]}...")
                
                # Salva na mem√≥ria da sess√£o
                if session_id:
                    if session_id not in self.memories:
                        self.memories[session_id] = []
                    self.memories[session_id].append({"role": "user", "content": prompt})
                    self.memories[session_id].append({"role": "assistant", "content": content})
                    self._trim_memory(session_id)
                
                return content.strip()
                
        except aiohttp.ClientError as e:
            logger.error(f"‚ùå Network error: {type(e).__name__}: {str(e)}")
            return self._get_fallback_response(prompt)
            
        except asyncio.TimeoutError:
            logger.error(f"‚ùå Timeout ap√≥s {self.timeout}s")
            return self._get_fallback_response(prompt)
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao gerar resposta: {type(e).__name__}: {str(e)}")
            logger.error(traceback.format_exc())
            return self._get_fallback_response(prompt)
    
    def _get_fallback_response(self, prompt: str) -> str:
        """Resposta fallback melhorada quando LLM n√£o est√° dispon√≠vel"""
        logger.info(f"üîÑ Using fallback response for: {prompt[:50]}...")
        prompt_lower = prompt.lower()
        # Respostas mais naturais e variadas
        greetings = ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa", "eae", "e ai"]
        if any(word in prompt_lower for word in greetings):
            responses = [
                "Oi! Tudo bem? üòä Como posso te ajudar hoje?",
                "Opa! Que bom te ver por aqui! O que voc√™ precisa?",
                "Oi oi! Como voc√™ t√°? Precisa de alguma coisa?",
                "Hey! Tudo certo? Me conta como posso ajudar!",
                "Ol√°! Seja bem-vindo(a)! Em que posso ser √∫til?"
            ]
            import random
            return random.choice(responses)
        elif any(word in prompt_lower for word in ["menu", "op√ß√µes", "opcoes", "ajuda", "o que voc√™ faz", "comandos"]):
            return """Claro! Eu posso te ajudar com v√°rias coisas:

üìä *Dados e Relat√≥rios* - Vendas, m√©tricas, dashboards
üõ†Ô∏è *Suporte T√©cnico* - Problemas, erros, d√∫vidas
üìÖ *Agendamentos* - Reuni√µes, compromissos
üí¨ *Bate-papo* - Qualquer outra coisa!

O que voc√™ precisa? üòä"""
        elif any(word in prompt_lower for word in ["dados", "relat√≥rio", "relatorio", "vendas", "dashboard", "m√©trica", "metrica", "kpi", "analise", "an√°lise"]):
            return """Legal! Vou puxar essas informa√ß√µes pra voc√™! üìä

Voc√™ quer ver:
- Vendas do m√™s?
- Comparativo com m√™s anterior?
- Performance geral?
- Ou algum dado espec√≠fico?

Me conta o que precisa!"""
        elif any(word in prompt_lower for word in ["erro", "problema", "bug", "n√£o funciona", "nao funciona", "travou", "lento", "falha"]):
            return """Poxa, que chato! Vamos resolver isso juntos üõ†Ô∏è

Me conta:
- O que aconteceu exatamente?
- Quando come√ßou o problema?
- J√° tentou reiniciar?

Com essas infos consigo te ajudar melhor!"""
        elif any(word in prompt_lower for word in ["agendar", "marcar", "reuni√£o", "reuniao", "hor√°rio", "horario", "agenda"]):
            return """üìÖ Vamos agendar!

Me diz:
- Que tipo de compromisso?
- Qual dia seria melhor?
- Tem hor√°rio preferido?

Vou verificar a disponibilidade pra voc√™!"""
        elif any(word in prompt_lower for word in ["obrigado", "obrigada", "valeu", "thanks", "brigado", "agrade√ßo"]):
            responses = [
                "Por nada! Sempre que precisar, t√¥ aqui! üòä",
                "Imagina! Foi um prazer ajudar! ü§ó",
                "Que isso! Conta comigo sempre! ‚ú®",
                "De nada! Volte sempre que precisar!"
            ]
            import random
            return random.choice(responses)
        elif any(word in prompt_lower for word in ["tchau", "at√©", "ate", "adeus", "bye", "xau", "flw", "falou"]):
            responses = [
                "Tchau! Foi √≥timo falar com voc√™! At√© mais! üëã",
                "At√© logo! Se cuida! üòä",
                "Valeu pela conversa! At√© a pr√≥xima! ‚ú®",
                "Tchau tchau! Volte sempre! ü§ó"
            ]
            import random
            return random.choice(responses)
        elif any(word in prompt_lower for word in ["teste", "testando", "test"]):
            return "üß™ Teste recebido! Estou funcionando perfeitamente! Como posso ajudar?"
        else:
            # Resposta gen√©rica mais natural
            responses = [
                "Hmm, n√£o entendi muito bem... Pode me explicar de outro jeito? üòä",
                "Opa, acho que n√£o captei. Pode dar mais detalhes?",
                "Desculpa, n√£o entendi direito. Voc√™ quer dados, suporte ou marcar algo?",
                "Poxa, n√£o peguei bem o que voc√™ precisa. Me conta mais?",
                "Interessante! Mas n√£o entendi completamente. Pode elaborar um pouco mais?"
            ]
            import random
            return random.choice(responses)
    
    async def classify_intent(self, message: str, session_id: str = None) -> Dict[str, Any]:
        """Classifica inten√ß√£o com fallback robusto"""
        logger.debug(f"üéØ Classifying intent for: {message[:50]}...")
        
        if self.is_initialized and self.session:
            system_prompt = """Voc√™ √© um classificador de inten√ß√µes. Analise a mensagem e classifique em uma destas categorias:
- "reception": sauda√ß√µes, cumprimentos, despedidas
- "data_query": dados, relat√≥rios, m√©tricas, an√°lises
- "technical_support": problemas, erros, bugs, suporte
- "scheduling": agendamentos, reuni√µes, calend√°rio
- "general_chat": outros assuntos

Responda APENAS em JSON: {"intent": "categoria", "confidence": 0.0-1.0, "reasoning": "breve explica√ß√£o"}"""

            try:
                response = await self.generate_response(
                    f"Classifique esta mensagem: '{message}'",
                    system_prompt,
                    session_id,
                    temperature=0.3,
                    max_tokens=150
                )
                
                logger.debug(f"Classification response: {response}")
                
                # Tenta extrair JSON da resposta
                response = response.strip()
                
                # Remove formata√ß√£o markdown se houver
                if "```json" in response:
                    response = response.split("```json")[1].split("```")[0].strip()
                elif "```" in response:
                    response = response.split("```")[1].split("```")[0].strip()
                
                # Encontra o JSON na resposta
                start = response.find('{')
                end = response.rfind('}') + 1
                
                if start != -1 and end > start:
                    json_str = response[start:end]
                    result = json.loads(json_str)
                    
                    # Valida estrutura
                    if all(key in result for key in ["intent", "confidence"]):
                        result["confidence"] = float(result["confidence"])
                        logger.info(f"‚úÖ Intent classified: {result['intent']} (confidence: {result['confidence']})")
                        return result
                
                logger.warning("Failed to parse LLM classification response, using fallback")
                        
            except Exception as e:
                logger.error(f"‚ùå Erro na classifica√ß√£o LLM: {type(e).__name__}: {str(e)}")
        
        # Fallback com classifica√ß√£o por palavras-chave
        logger.info("üîÑ Using keyword-based classification")
        return self._classify_by_keywords(message)
    
    def _classify_by_keywords(self, message: str) -> Dict[str, Any]:
        """Classifica√ß√£o fallback por palavras-chave melhorada"""
        message_lower = message.lower()
        
        # Padr√µes mais abrangentes
        patterns = {
            "reception": {
                "keywords": ["oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", "hey", "opa", 
                           "e ai", "eai", "fala", "al√¥", "alo", "prezado", "caro", "tchau", "at√©",
                           "obrigado", "valeu", "flw", "falou"],
                "confidence": 0.95
            },
            "data_query": {
                "keywords": ["relat√≥rio", "relatorio", "dados", "dashboard", "vendas", "faturamento", 
                           "m√©trica", "metrica", "kpi", "n√∫meros", "numeros", "estat√≠stica", 
                           "estatistica", "an√°lise", "analise", "gr√°fico", "grafico", "planilha",
                           "excel", "csv", "exportar", "resultado", "performance", "indicador"],
                "confidence": 0.85
            },
            "technical_support": {
                "keywords": ["erro", "problema", "bug", "n√£o funciona", "nao funciona", "travou", 
                           "lento", "parou", "ajuda t√©cnica", "suporte", "falha", "crash", "down",
                           "offline", "timeout", "conex√£o", "conexao", "acesso negado", "senha",
                           "login", "autentica√ß√£o", "autenticacao", "permiss√£o", "permissao"],
                "confidence": 0.85
            },
            "scheduling": {
                "keywords": ["agendar", "marcar", "reuni√£o", "reuniao", "hor√°rio", "horario", 
                           "calend√°rio", "calendario", "compromisso", "disponibilidade", "agenda",
                           "remarcar", "cancelar", "adiar", "confirmar", "meeting", "call",
                           "videoconfer√™ncia", "videoconferencia"],
                "confidence": 0.85
            }
        }
        
        # Verifica cada padr√£o
        for intent, pattern in patterns.items():
            if any(keyword in message_lower for keyword in pattern["keywords"]):
                reasoning = f"Detectada palavra-chave relacionada a {intent}"
                logger.info(f"‚úÖ Keyword match for intent: {intent}")
                return {
                    "intent": intent,
                    "confidence": pattern["confidence"],
                    "reasoning": reasoning
                }
        
        # Se n√£o encontrar padr√£o claro, classifica como general_chat
        logger.info("‚ÑπÔ∏è No clear pattern found, classifying as general_chat")
        return {
            "intent": "general_chat",
            "confidence": 0.5,
            "reasoning": "Nenhum padr√£o espec√≠fico detectado"
        }
    
    def _trim_memory(self, session_id: str, max_messages: int = 10):
        """Mant√©m apenas as √∫ltimas N mensagens na mem√≥ria"""
        if session_id in self.memories and len(self.memories[session_id]) > max_messages:
            self.memories[session_id] = self.memories[session_id][-max_messages:]
            logger.debug(f"Trimmed memory for session {session_id} to {max_messages} messages")
    
    async def cleanup(self):
        """Limpa recursos"""
        logger.info("üßπ Cleaning up LLM Service...")
        
        if self.session:
            await self.session.close()
            
        self.memories.clear()
        self.is_initialized = False
        
        logger.info("‚úÖ LLM Service cleaned up")

    async def get_service_status(self) -> Dict[str, Any]:
        """Retorna status detalhado do servi√ßo LLM"""
        try:
            status = {
                "status": "online" if self.is_initialized else "offline",
                "initialized": self.is_initialized,
                "ollama_url": self.ollama_url,
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "timeout": self.timeout,
                "timestamp": datetime.now().isoformat(),
                "memory_sessions": len(self.memories),
                "connection_error": self.connection_error,
                "last_test": {
                    "time": self.last_test_time.isoformat() if self.last_test_time else None,
                    "result": self.last_test_result
                }
            }
            
            # Testa conex√£o atual se inicializado
            if self.is_initialized and self.session:
                try:
                    # Teste r√°pido de conectividade
                    test_start = datetime.now()
                    async with self.session.get(
                        f"{self.ollama_url}/api/tags", 
                        timeout=aiohttp.ClientTimeout(total=2)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            models = [m.get('name', '') for m in data.get('models', [])]
                            status["available_models"] = models
                            status["model_available"] = self.model in models
                            status["connectivity"] = "connected"
                            status["ping_ms"] = int((datetime.now() - test_start).total_seconds() * 1000)
                        else:
                            status["connectivity"] = "error"
                            status["connectivity_error"] = f"HTTP {response.status}"
                except asyncio.TimeoutError:
                    status["connectivity"] = "timeout"
                except Exception as e:
                    status["connectivity"] = "error"
                    status["connectivity_error"] = str(e)
            else:
                status["connectivity"] = "not_initialized"
            
            return status
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }